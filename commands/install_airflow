#!/usr/bin/env bash

#
# install_airflow
#
# Install Airflow, with all Airflow packages

# Set the default arguments
dags_folder="/opt/airflow/dags"
postgres_url=
use_celery=false
redis_url=

# Keep track of the original arguments passed to this script
original_arguments=()
for param in "$@"; do
  original_arguments+=("$param")
done

# Explain how to use this script
usage(){
	cat <<- EOF

	Usage: install_airflow [options]

	Options:
		-f, --dags_folder <path>                The path where Airflow will look for DAGs; e.g., /dags
    -p, --postgres_url <url>                Use PostgreSQL, with this connection string; e.g., user:pass@host/db
    -c, --celery                            Use Celery to distribute Airflow execution
    -r, --redis_url <url>                   The Redis connection string; e.g., pass@host:port/db_number
		-h, --help                              This message

	EOF
}

# If the user asks for help, print out the usage
set -- "${original_arguments[@]}"
while [ "$1" != "" ]; do
  case $1 in
    -h | --help )
      usage
      exit
      ;;
    *)
      shift
      ;;
  esac
done

# Capture the command line arguements from the user
set -- "${original_arguments[@]}"
while [ "$1" != "" ]; do
  case $1 in
    -f | --dags_folder )
      shift
      dags_folder=$1
      shift
      ;;
    -p | --postgres_url )
      shift
      postgres_url=$1
      shift
      ;;
    -c | --celery )
      use_celery=true
      shift
      ;;
    -r | --redis_url )
      shift
      redis_url=$1
      shift
      ;;
    *)
      usage
      exit 1
  esac
done

# Ensure we have the DAGs folder before we proceed
if [ -z $dags_folder ]; then
  echo "The DAGs folder must be set; use -d or --dags_folder"
  exit 1
fi
# If using Celery, Postgres and Redis must be enabled
if [ "$use_celery" == true ] && ([ -z $postgres_url ] || [ -z $redis_url ]); then
  echo "When using Celery, the PostgreSQL connection URL and Redis connection URL must be specified; use -p or --postgres_url for PostgreSQL and -r or --redis_url for Redis"
  exit 1
fi
# You can't use Redis unless Celery is on
if [ -n "$redis_url" ] && [ "$use_celery" == false ]; then
  echo "Airflow cannot use Redis if Celery is disabled; turn on Celery with -c or --celery or stop setting Redis with -r or --redis_url"
  exit 1 
fi

# Set up a virtualenv for Airflow
cd /opt/virtualenvs
virtualenv airflow

# Install libffi-dev
# (required for Airflow's crypto package)
sudo rpm -i http://195.220.108.108/linux/centos/7.3.1611/os/x86_64/Packages/libffi-devel-3.0.13-18.el7.x86_64.rpm
sudo yum -y install libffi-devel

# Make Airflow's home directory
sudo mkdir /opt/airflow
sudo chown centos:centos /opt/airflow
echo export AIRFLOW_HOME=/opt/airflow >> /home/centos/.bash_profile
source /home/centos/.bash_profile

# Preempt the boilerplate Airflow configuraiton file with our own
cat << EOF | sudo tee /opt/airflow/airflow.cfg
[core]
airflow_home = /opt/airflow
dags_folder = $dags_folder
base_log_folder = /opt/airflow/logs
EOF

if [ "$use_celery" == true ]; then
	cat <<-EOF | sudo tee -a /opt/airflow/airflow.cfg
		executor = CeleryExecutor
	EOF
else
	cat <<-EOF | sudo tee -a /opt/airflow/airflow.cfg
		executor = SequentialExecutor
	EOF
fi

if [ -z $postgres_url ]; then
	cat <<-EOF | sudo tee -a /opt/airflow/airflow.cfg
		sql_alchemy_conn = sqlite:////opt/airflow/airflow.db	
	EOF
else
	cat <<-EOF | sudo tee -a /opt/airflow/airflow.cfg
		sql_alchemy_conn = postgresql+psycopg2://`echo $postgres_url`
		sql_alchemy_pool_size = 5
		sql_alchemy_pool_recycle = 3600
	EOF
fi

cat << EOF | sudo tee -a /opt/airflow/airflow.cfg
parallelism = 32
dag_concurrency = 16
load_examples = False
plugins_folder = /opt/airflow/plugins
fernet_key = `cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 32 | head -n 1`
unit_test_mode = False

[cli]
api_client = airflow.api.client.local_client
endpoint_url = http://localhost:8080

[api]
auth_backend = airflow.api.auth.backend.default

[operators]
default_owner = Airflow
default_cpus = 1
default_ram = 512
default_disk = 512
default_gpus = 0

[webserver]
base_url = http://localhost:8080
web_server_host = 0.0.0.0
web_server_port = 8080
worker_refresh_batch_size = 1
worker_refresh_interval = 30
secret_key = `cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 32 | head -n 1`
workers = 4
worker_class = sync

[scheduler]
job_heartbeat_sec = 5
scheduler_heartbeat_sec = 5
run_duration = -1
min_file_process_interval = 0
dag_dir_list_interval = 300
print_stats_interval = 30
child_process_log_directory = /opt/airflow/logs/scheduler
scheduler_zombie_task_threshold = 300
catchup_by_default = True
max_threads = 2
authenticate = False

EOF

if [ "$use_celery" == true ]; then
	cat <<-EOF | sudo tee -a /opt/airflow/airflow.cfg
		[celery]
		celery_app_name = airflow.executors.celery_executor
		celeryd_concurrency = 16
		worker_log_server_port = 8793
		broker_url = redis://:`echo $redis_url`
		celery_result_backend = redis://:`echo $redis_url`
		flower_host = 0.0.0.0
		flower_port = 5555
		default_queue = default

	EOF
fi

cat << EOF | sudo tee -a /opt/airflow/airflow.cfg
[github_enterprise]
api_rev = v3

EOF

sudo chown centos:centos /opt/airflow/airflow.cfg

# Install Airflow, with all Airflow packages
cd /opt/airflow
/opt/virtualenvs/airflow/bin/pip install airflow[all]

# Install Celery, so that we can distribute Airflow work
/opt/virtualenvs/airflow/bin/pip install celery[redis]

# Install Flower, so Celery can be monitored
/opt/virtualenvs/airflow/bin/pip install flower

# Initialize the Airflow database
/opt/virtualenvs/airflow/bin/airflow initdb

# Add Airflow's binary directory to the $PATH, so that Airflow can find the binaries it depends on, like gunicorn
echo PATH=\$PATH:/opt/virtualenvs/airflow/bin >> ~/.bash_profile
source /home/centos/.bash_profile

# Make scripts that can start Airflow the webserver, scheduler, worker, and Flower with
# all environmental variables (Perfect for use in non-interactive shells without
# Virtualenv's activate)
cat << EOF | sudo tee /opt/airflow/start_airflow_webserver
#!/usr/bin/env bash

VIRTUAL_ENV="/opt/virtualenvs/airflow" PATH="\$VIRTUAL_ENV/bin:\$PATH" AIRFLOW_HOME=/opt/airflow /opt/virtualenvs/airflow/bin/airflow webserver -p 8080

EOF
sudo chown centos:centos /opt/airflow/start_airflow_webserver
chmod +x /opt/airflow/start_airflow_webserver

cat << EOF | sudo tee /opt/airflow/start_airflow_scheduler
#!/usr/bin/env bash

VIRTUAL_ENV="/opt/virtualenvs/airflow" PATH="\$VIRTUAL_ENV/bin:\$PATH" AIRFLOW_HOME=/opt/airflow /opt/virtualenvs/airflow/bin/airflow scheduler --num_runs 20

EOF
sudo chown centos:centos /opt/airflow/start_airflow_scheduler
chmod +x /opt/airflow/start_airflow_scheduler

cat << EOF | sudo tee /opt/airflow/start_airflow_worker
#!/usr/bin/env bash

VIRTUAL_ENV="/opt/virtualenvs/airflow" PATH="\$VIRTUAL_ENV/bin:\$PATH" AIRFLOW_HOME=/opt/airflow /opt/virtualenvs/airflow/bin/airflow worker -p 8080

EOF
sudo chown centos:centos /opt/airflow/start_airflow_worker
chmod +x /opt/airflow/start_airflow_worker

cat << EOF | sudo tee /opt/airflow/start_airflow_flower_webserver
#!/usr/bin/env bash

VIRTUAL_ENV="/opt/virtualenvs/airflow" PATH="\$VIRTUAL_ENV/bin:\$PATH" AIRFLOW_HOME=/opt/airflow /opt/virtualenvs/airflow/bin/airflow flower

EOF
sudo chown centos:centos /opt/airflow/start_airflow_flower_webserver
chmod +x /opt/airflow/start_airflow_flower_webserver

# N.B., this is how you start the Airflow web server
# /opt/airflow/start_airflow_webserver

# N.B., this is how you start the Airflow scheduler
# /opt/airflow/start_airflow_scheduler

# N.B., this is how you start the Airflow worker
# /opt/airflow/start_airflow_worker

# N.B., this is how you start the Flower worker
# /opt/airflow/start_airflow_flower_webserver
